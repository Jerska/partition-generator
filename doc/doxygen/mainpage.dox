/**
\mainpage Real-Time Score Generation

\section cont Context

This project is done by four EPITA students in collaboration with
Jellynote, who offer interactive web services revolving around
music sheet distribution and viewing, as well as help following
sheets while playing, Guitar Hero-style. You can access the Jellynote
website here: https://www.jellynote.com/en/.
The four students developing this program linked up with Jellynote
as part of a business-oriented project called PFEE, in which students
work for a company during their studies. All four of us are doing our
last year at EPITA, all as part of the SCIA major, which specializes
in Cognitive Science and Artificial Intelligence.

\section desc Description

 The aim of this project is to add to the multiple services offered by Jellynote, 
 in the form of a real-time score genrator: The user will play music in front 
 of its computer, and will be able to see the notations pop up on the initially blank
sheet as he plays on. This project is more of a proof-of-concept than a fully
fledged add-on to Jellynote. Our main objective is to detect simple notes, played
one after the other.

The program is browser-based and written using these languages and tools:
- C++ for the bulk of the program.
- emscipten for generating JavaScript code from the C++ one.
- JavaScipt and Flash for browser-based sound recording and microphone access.
- Coffee to use nodejs.
- Lilypond for generating scores.

The main "parts" or "modules" of the project are :
- Sound retrieval and noise detection and cancelling.
- Find if a note has been played, and if so, which one it is. This is done
using a Fast-Fourier Transform on the entry signal, along
with multiple other techniques explained later in the documentation.
- Generation of the score using Lilypond. The generation has to be fast enough 
to satisfy real-time constraints.

\section comp Compiling and running the program

In order for us to be able to run a test suit, our program is seperated in two parts:
- The main part, the real-time score generator inside a browser, is to be compiled the following way, at the root of the project directory:
    - make
    - ./run.sh   note: it is recommended to launch this in its separate cmd tab or window, as it may be difficult to stop this from running.
    - access http://localhost:3000/ in your browser

- The second part mainly compiles the same files as the first, the difference being that this part is geered towards unit tests. Indeed, it takes .wav files as an entry instead of the microphone, enabling us to test individual notes and see if they return the correct note. To run the test suite, these are the steps to follow:
    - cd test/
    - make
    - python tester.py music_notes/low_octaves/ music_notes/mid_octaves/ music_notes/high_octaves/

This will launch the test suite on every file in the three directories. The reason we have seperated these .wav files in three different directories is because one of the main obstacles we encountered was making the program detect low, mid and high octaves at the same time, and as a result it is interesting to be able to compare performance between these different subtypes of notes.

As of the last installment of the project, we had a 96% rate of individual note recognition.

\section tec Note Detection

Here we will overview the different techniques used to retrieve information
from the recorded sound:
    - The first part is retrieving the sound from the microphone (Or from .wav files, the process being the same). This is done in Flash ... TODO: DUMONT. The sound is retrieved by intervals of ~10ms, and each window is then sent to the c++ part to be processed. This part is handled by the following files:
        - *.js
        - TODO: DUMONT: add the files and very quick explanation of what they do

    - A Fast Fourier transform is then applied to these 10ms of sound. This switches the data from time-based to frequency-based, which enables us to analize these frequencies and extract a note if it exists.
    - Compute magnitude: The magnitude of the signal is obtained by computing the square root of both the real and imagniray part of the signal resulting from the fft. 
    - Compute spectre: The spectrum of the signal is computed from its magnitude by converting it to a logarithmic scale so that it will follow human ear behavior which perceives sounds on a logarithmic scale.
    - HPS: The HPS (Harmonic Product Spectrum) is the algorithm used to detect the fundamental frequency of a note played from the spectrum of the signal. Each note played from an instrument has several frequency peaks and the HPS allows us to determine which of those peaks corresponds to the note actually being played. 
    - Note detection: The note is detected once the HPS has been applied to the signal. It corresponds to the frequency of the highest peak of the resulting signal.
    - OnSet detection: The Onset detection is performed by keeping track of the amplitude of several of the previous results of the signal processing. If it is determined that there is a peak in those amplitudes (understand one amplitude higher than its two neighboors), and if the amplitude of the note played is higher than a fixed threshold, then we consider that the results of the signal processing correspond indeed to a newly played note.   



\section disp Displaying the score

The last part is of course having to actually display the score in real-time. Jellynote kindly handed us a bit of code to help us with displaying notes in real-time, since this is not natively handled by Lillypond. We had to modify parts of that code to make it usable for our project. The code for this is situated in two files in the directory ./src/js/:
- partition.js: link between our note recognition module and the score display module.
- jellyscore.js: actual code to display and update the score in the web page.

\section Conclusion

This project has been very enriching for the four of us. We have managed to have a satisfying note recognition and display of the score when it comes to playing single consecutive notes, which was our main objective at the start of tis project.

As a last word, we will enumerate some ways that we think this project could be enhanced:
- Integrating it in Jellynote's environment (enabling sharing, post-play modification of the score, and other features)
- Recognition of chords and other musical effects.  

\authors Matthieu Guyot de Camy
\authors Quentin Cavali√©
\authors Julien Gonzalez
\authors Matthieu Dumont
*/